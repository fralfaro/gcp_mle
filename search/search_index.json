{"config":{"lang":["es"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u2601\ufe0f Bienvenido a Machine Learning con GCP","text":""},{"location":"#contenidos-del-curso","title":"\ud83d\udcda Contenidos del Curso","text":"<p>IAM y Seguridad</p><p>Aprende a gestionar identidades, roles, permisos y cuentas de servicio en GCP para proteger tus proyectos de ML.</p> <p>Almacenamiento de Datos</p><p>Conoce c\u00f3mo usar Cloud Storage y BigQuery para almacenar datasets estructurados y no estructurados a escala.</p> <p>Procesamiento de Datos</p><p>Usa Dataflow, Dataproc y BigQuery para transformar grandes vol\u00famenes de datos de manera eficiente y escalable.</p> <p>Automatizaci\u00f3n y Pipelines</p><p>Dise\u00f1a y orquesta flujos de trabajo ML con Vertex Pipelines, Workflows y Composer.</p> <p>Modelos y Entrenamiento</p><p>Entrena modelos en Vertex AI, explora AutoML y despliega soluciones de ML en producci\u00f3n.</p> <p>Extras</p><p>M\u00e1s sobre otros servicios de GCP, como Data Studio, Looker y herramientas de visualizaci\u00f3n.</p>"},{"location":"unit1/","title":"\ud83d\udcd8 Unidad 1: IAM y Seguridad","text":"<p>Esta unidad te introduce a la gesti\u00f3n de identidades y accesos en Google Cloud, un aspecto fundamental para proteger tus proyectos de Machine Learning.</p>"},{"location":"unit1/#capitulos","title":"\ud83d\udcc4 Cap\u00edtulos","text":"Archivo T\u00edtulo Descripci\u00f3n breve <code>iam/intro.md</code> \ud83d\udd10 Introducci\u00f3n a IAM Qu\u00e9 es IAM, por qu\u00e9 es importante y c\u00f3mo se aplica en GCP. <code>iam/roles.md</code> \ud83c\udfad Roles y pol\u00edticas Tipos de roles (b\u00e1sicos, predefinidos, personalizados) y c\u00f3mo asignarlos. <code>iam/service-accounts.md</code> \ud83e\uddfe Cuentas de servicio Qu\u00e9 son, c\u00f3mo crearlas y c\u00f3mo se usan para automatizar procesos. <code>iam/best-practices.md</code> \u2705 Buenas pr\u00e1cticas de acceso Consejos y patrones recomendados para aplicar el principio de menor privilegio."},{"location":"unit1/best-practices/","title":"Buenas Pr\u00e1cticas","text":""},{"location":"unit1/best-practices/#introduccion","title":"Introducci\u00f3n","text":"<p>Aplicar buenas pr\u00e1cticas en IAM es fundamental para construir soluciones seguras, mantenibles y auditables en Google Cloud, especialmente cuando se trabaja con datos sensibles o se automatizan flujos de Machine Learning.</p> <p>En este cap\u00edtulo aprender\u00e1s principios y recomendaciones clave para mejorar la seguridad y la gobernanza de accesos.</p>"},{"location":"unit1/best-practices/#por-que-son-importantes-las-buenas-practicas","title":"\u00bfPor qu\u00e9 son importantes las buenas pr\u00e1cticas?","text":"<p>IAM mal configurado puede abrir puertas a:</p> <ul> <li>\ud83d\uded1 P\u00e9rdida de datos por accesos indebidos.</li> <li>\ud83d\udea8 Vulnerabilidades en pipelines automatizados.</li> <li>\ud83e\udd2f Dificultad para auditar qui\u00e9n hizo qu\u00e9 y cu\u00e1ndo.</li> <li>\ud83e\udde9 Problemas de escalabilidad y colaboraci\u00f3n en equipos grandes.</li> </ul> <p>Aplicar buenas pr\u00e1cticas ayuda a prevenir estos riesgos desde el inicio del proyecto.</p>"},{"location":"unit1/best-practices/#principales-recomendaciones","title":"Principales recomendaciones","text":""},{"location":"unit1/best-practices/#1-aplica-el-principio-de-menor-privilegio","title":"\ud83d\udd12 1. Aplica el principio de menor privilegio","text":"<ul> <li>Otorga solo los permisos necesarios para realizar una tarea.</li> <li>Revisa y elimina roles innecesarios o excesivos.</li> <li>Evita el uso de <code>roles/owner</code>, <code>roles/editor</code> o <code>roles/viewer</code> en producci\u00f3n.</li> </ul>"},{"location":"unit1/best-practices/#2-usa-cuentas-de-servicio-por-flujo-de-trabajo","title":"\ud83d\udc65 2. Usa cuentas de servicio por flujo de trabajo","text":"<ul> <li>Crea una cuenta por pipeline, funci\u00f3n o proceso automatizado.</li> <li>Asigna a cada cuenta solo los roles que necesita.</li> <li>Evita reutilizar cuentas en m\u00faltiples contextos.</li> </ul>"},{"location":"unit1/best-practices/#3-prefiere-roles-predefinidos","title":"\ud83e\uddf1 3. Prefiere roles predefinidos","text":"<ul> <li>Usa roles como <code>roles/aiplatform.user</code>, <code>roles/storage.objectViewer</code>, etc.</li> <li>Solo crea roles personalizados si los predefinidos no satisfacen tu caso.</li> <li>Documenta cualquier rol personalizado que utilices.</li> </ul>"},{"location":"unit1/best-practices/#4-administra-el-acceso-por-proyecto","title":"\ud83d\udce6 4. Administra el acceso por proyecto","text":"<ul> <li>Organiza tus proyectos por entorno (ej. <code>ml-dev</code>, <code>ml-prod</code>).</li> <li>Otorga roles a nivel de proyecto, no en recursos individuales si no es necesario.</li> <li>Usa etiquetas y carpetas para separar ambientes o l\u00edneas de negocio.</li> </ul>"},{"location":"unit1/best-practices/#5-revisa-politicas-periodicamente","title":"\ud83e\uddfe 5. Revisa pol\u00edticas peri\u00f3dicamente","text":"<ul> <li>Usa <code>gcloud projects get-iam-policy</code> para revisar accesos.</li> <li>Implementa auditor\u00edas trimestrales o autom\u00e1ticas.</li> <li>Usa herramientas como Policy Analyzer o IAM Recommender.</li> </ul> <pre><code>gcloud projects get-iam-policy mi-proyecto-id\n</code></pre>"},{"location":"unit1/best-practices/#6-evita-la-dependencia-de-credenciales-locales","title":"\ud83d\udd01 6. Evita la dependencia de credenciales locales","text":"<ul> <li>Usa autenticaci\u00f3n autom\u00e1tica con cuentas de servicio en GKE, Cloud Run o Vertex.</li> <li>Si necesitas credenciales <code>.json</code>, usa secretos y no los incluyas en el repositorio.</li> </ul>"},{"location":"unit1/best-practices/#herramientas-utiles","title":"Herramientas \u00fatiles","text":"Herramienta Uso principal IAM Policy Analyzer Detecta roles innecesarios o riesgosos IAM Recommender Sugerencias autom\u00e1ticas de permisos m\u00ednimos Audit Logs Revisi\u00f3n de acciones hist\u00f3ricas Cloud Asset Inventory Inventario completo de roles y recursos"},{"location":"unit1/best-practices/#checklist-de-implementacion-segura","title":"Checklist de implementaci\u00f3n segura","text":"<ul> <li>[ ] Cada cuenta de servicio tiene un prop\u00f3sito claro y documentado.</li> <li>[ ] No se usan roles amplios como <code>Editor</code> u <code>Owner</code> en producci\u00f3n.</li> <li>[ ] Todas las pol\u00edticas est\u00e1n revisadas al menos una vez al trimestre.</li> <li>[ ] El acceso a recursos de ML (datasets, modelos) est\u00e1 auditado.</li> <li>[ ] Las claves de cuentas de servicio est\u00e1n protegidas y rotadas peri\u00f3dicamente.</li> </ul>"},{"location":"unit1/best-practices/#recursos-adicionales","title":"Recursos adicionales","text":"<ul> <li>IAM Recommender</li> <li>IAM Policy Troubleshooter</li> <li>Cloud Audit Logs</li> </ul>"},{"location":"unit1/best-practices/#conclusion","title":"Conclusi\u00f3n","text":"<p>Una buena configuraci\u00f3n de IAM no solo mejora la seguridad, sino que tambi\u00e9n facilita la automatizaci\u00f3n, la colaboraci\u00f3n y el monitoreo en tus flujos de Machine Learning.</p> <p>En la pr\u00f3xima unidad aprender\u00e1s c\u00f3mo almacenar, consultar y preparar datos usando Cloud Storage y BigQuery.</p>"},{"location":"unit1/intro/","title":"Introducci\u00f3n a IAM","text":""},{"location":"unit1/intro/#introduccion","title":"Introducci\u00f3n","text":"<p>El sistema Identity and Access Management (IAM) de Google Cloud permite definir qui\u00e9n puede hacer qu\u00e9 sobre qu\u00e9 recurso. En proyectos de Machine Learning, esto es esencial para proteger datos, controlar accesos a modelos y automatizar procesos de manera segura.</p> <p>En este cap\u00edtulo, aprender\u00e1s los componentes principales de IAM, c\u00f3mo aplicarlos y por qu\u00e9 es fundamental entender esta arquitectura para cualquier proyecto en la nube.</p>"},{"location":"unit1/intro/#por-que-es-importante-entender-iam","title":"\u00bfPor qu\u00e9 es importante entender IAM?","text":"<p>Comprender c\u00f3mo funciona IAM te permitir\u00e1:</p> <ul> <li>Prevenir accesos no autorizados a tus datos o modelos.</li> <li>Automatizar flujos de trabajo de manera segura con cuentas de servicio.</li> <li>Implementar el principio de menor privilegio y reducir riesgos.</li> <li>Diagnosticar errores de permisos y optimizar configuraciones de seguridad.</li> </ul>"},{"location":"unit1/intro/#componentes-principales-de-iam","title":"Componentes principales de IAM","text":"<p>Google Cloud aplica IAM en una jerarqu\u00eda: organizaci\u00f3n \u2192 carpeta \u2192 proyecto \u2192 recurso.</p> <p>A continuaci\u00f3n, los elementos clave del sistema:</p>"},{"location":"unit1/intro/#1-principals-identidades","title":"1. Principals (Identidades)","text":"<ul> <li>Representan a quien accede a los recursos.</li> <li>Pueden ser: usuarios (<code>user:</code>), grupos (<code>group:</code>), cuentas de servicio (<code>serviceAccount:</code>) o dominios.</li> </ul>"},{"location":"unit1/intro/#2-roles","title":"2. Roles","text":"<ul> <li>Definen un conjunto de permisos.</li> <li>Tipos:</li> <li>B\u00e1sicos: Owner, Editor, Viewer.</li> <li>Predefinidos: roles espec\u00edficos por servicio (ej. <code>roles/storage.objectViewer</code>).</li> <li>Personalizados: creados por el usuario.</li> </ul>"},{"location":"unit1/intro/#3-permisos","title":"3. Permisos","text":"<ul> <li>Son acciones sobre recursos, como:</li> <li><code>storage.buckets.get</code></li> <li><code>bigquery.datasets.create</code></li> </ul>"},{"location":"unit1/intro/#4-politicas","title":"4. Pol\u00edticas","text":"<ul> <li>Asocian uno o m\u00e1s roles a uno o m\u00e1s principals.</li> <li>Se aplican a un recurso espec\u00edfico (proyecto, bucket, dataset, etc.).</li> </ul>"},{"location":"unit1/intro/#diagrama-logico-de-iam-en-gcp","title":"Diagrama l\u00f3gico de IAM en GCP","text":"<pre><code>Organizaci\u00f3n\n \u2514\u2500\u2500 Carpeta (opcional)\n      \u2514\u2500\u2500 Proyecto\n           \u2514\u2500\u2500 Recurso (bucket, modelo, tabla...)\n</code></pre> <p>Las pol\u00edticas se heredan hacia abajo. Si das un rol a nivel de proyecto, todos los recursos dentro de ese proyecto lo heredar\u00e1n.</p>"},{"location":"unit1/intro/#ejemplo-practico","title":"Ejemplo pr\u00e1ctico","text":"<p>Otorgar permisos de lectura a un bucket de Storage para un usuario:</p> <pre><code>gcloud projects add-iam-policy-binding mi-proyecto-id \\\n  --member=\"user:persona@ejemplo.com\" \\\n  --role=\"roles/storage.objectViewer\"\n</code></pre> <p>Este comando: - A\u00f1ade una pol\u00edtica de IAM al proyecto. - Aplica el rol de visualizador de objetos de Storage. - Asocia ese rol al usuario especificado.</p>"},{"location":"unit1/intro/#iam-en-proyectos-de-ml","title":"IAM en proyectos de ML","text":"<p>IAM se usa en cada etapa del ciclo de vida de Machine Learning:</p> <ul> <li>\ud83d\udd12 Acceso a datasets en Storage o BigQuery.</li> <li>\ud83e\udde0 Entrenamiento de modelos con Vertex AI (y cuentas de servicio dedicadas).</li> <li>\ud83d\ude80 Despliegue de modelos y control de endpoints.</li> <li>\ud83e\udd16 Automatizaci\u00f3n con Workflows, Cloud Functions, y pipelines de Vertex AI.</li> </ul>"},{"location":"unit1/intro/#recomendaciones-clave","title":"Recomendaciones clave","text":"<ul> <li>\u2705 Aplica el principio de menor privilegio.</li> <li>\ud83d\udd10 Usa cuentas de servicio separadas para pipelines y tareas autom\u00e1ticas.</li> <li>\ud83e\udde9 Prefiere roles predefinidos antes de crear personalizados.</li> <li>\ud83d\udcca Supervisa accesos con Cloud Audit Logs.</li> </ul>"},{"location":"unit1/intro/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Documentaci\u00f3n oficial de IAM</li> <li>Comando <code>gcloud</code> para IAM</li> <li>IAM para Vertex AI</li> </ul>"},{"location":"unit1/intro/#conclusion","title":"Conclusi\u00f3n","text":"<p>IAM es el primer paso para construir soluciones seguras y escalables en GCP. Con una buena gesti\u00f3n de accesos, no solo proteges tus datos y modelos, sino que tambi\u00e9n simplificas la automatizaci\u00f3n y el trabajo colaborativo.</p> <p>En el siguiente cap\u00edtulo exploraremos c\u00f3mo funcionan los roles y pol\u00edticas para asignar permisos de forma granular.</p>"},{"location":"unit1/roles/","title":"Roles y Pol\u00edticas","text":""},{"location":"unit1/roles/#introduccion","title":"Introducci\u00f3n","text":"<p>En IAM, los roles definen el conjunto de acciones (permisos) que una identidad puede realizar sobre un recurso. Las pol\u00edticas vinculan esos roles a identidades espec\u00edficas dentro de un alcance determinado, como un proyecto o bucket.</p> <p>Comprender esta relaci\u00f3n es clave para implementar accesos seguros, eficientes y escalables.</p>"},{"location":"unit1/roles/#por-que-es-importante-entender-los-roles","title":"\u00bfPor qu\u00e9 es importante entender los roles?","text":"<p>Los roles son la forma en que GCP controla el acceso. Usar el rol adecuado:</p> <ul> <li>Minimiza riesgos de seguridad.</li> <li>Mejora la gobernanza.</li> <li>Permite delegar tareas sin exponer servicios cr\u00edticos.</li> </ul>"},{"location":"unit1/roles/#tipos-de-roles-en-iam","title":"Tipos de roles en IAM","text":"<p>Google Cloud define tres tipos de roles:</p>"},{"location":"unit1/roles/#1-roles-basicos","title":"1. Roles B\u00e1sicos","text":"Rol Permisos Owner Control total sobre todos los recursos (incluye IAM). Editor Modifica recursos pero no gestiona IAM. Viewer Solo lectura sobre recursos. <p>\u2757 No recomendados para producci\u00f3n, ya que son muy amplios.</p>"},{"location":"unit1/roles/#2-roles-predefinidos","title":"2. Roles Predefinidos","text":"<ul> <li>Dise\u00f1ados para servicios espec\u00edficos.</li> <li>Ejemplos:</li> <li><code>roles/storage.objectViewer</code>: lectura de objetos en Cloud Storage.</li> <li><code>roles/bigquery.dataEditor</code>: edici\u00f3n de tablas en BigQuery.</li> <li><code>roles/ml.developer</code>: acceso a entrenamiento y despliegue en Vertex AI.</li> </ul> <p>\u2705 Recomendados para controlar el acceso de manera precisa y segura.</p>"},{"location":"unit1/roles/#3-roles-personalizados","title":"3. Roles Personalizados","text":"<ul> <li>Creados por el usuario.</li> <li>Permiten definir exactamente qu\u00e9 permisos incluir.</li> <li>\u00datiles cuando ning\u00fan rol predefinido se ajusta a las necesidades.</li> </ul> <pre><code>gcloud iam roles create customViewer \\\n  --project=mi-proyecto-id \\\n  --title=\"Custom Viewer\" \\\n  --permissions=\"storage.buckets.get,storage.objects.list\" \\\n  --stage=GA\n</code></pre>"},{"location":"unit1/roles/#politicas-iam","title":"Pol\u00edticas IAM","text":"<p>Una pol\u00edtica de IAM define qui\u00e9n tiene qu\u00e9 rol sobre qu\u00e9 recurso.</p>"},{"location":"unit1/roles/#estructura-simplificada","title":"Estructura simplificada:","text":"<pre><code>{\n  \"bindings\": [\n    {\n      \"role\": \"roles/storage.objectViewer\",\n      \"members\": [\n        \"user:usuario@ejemplo.com\"\n      ]\n    }\n  ]\n}\n</code></pre> <ul> <li><code>bindings</code>: lista de asignaciones.</li> <li><code>role</code>: el rol otorgado.</li> <li><code>members</code>: las identidades que lo reciben.</li> </ul>"},{"location":"unit1/roles/#ejemplo-practico-acceso-granular-a-bigquery","title":"Ejemplo pr\u00e1ctico: acceso granular a BigQuery","text":"<pre><code>gcloud projects add-iam-policy-binding mi-proyecto-id \\\n  --member=\"serviceAccount:mi-pipeline@mi-proyecto.iam.gserviceaccount.com\" \\\n  --role=\"roles/bigquery.dataViewer\"\n</code></pre> <p>Este comando permite que una cuenta de servicio acceda a datos de BigQuery, sin permitirle editar o borrar.</p>"},{"location":"unit1/roles/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>\ud83c\udfaf Usa roles predefinidos siempre que sea posible.</li> <li>\ud83d\udd10 Evita <code>Owner</code> y <code>Editor</code> en producci\u00f3n.</li> <li>\ud83e\uddea Usa cuentas de servicio separadas por flujo de trabajo.</li> <li>\ud83e\udde9 Usa etiquetas o nombres descriptivos para los roles personalizados.</li> <li>\ud83e\uddfe Documenta las pol\u00edticas aplicadas por recurso y prop\u00f3sito.</li> </ul>"},{"location":"unit1/roles/#diagrama-de-relacion","title":"Diagrama de relaci\u00f3n","text":"<pre><code>[Principal]\n   |\n   v\n[Rol asignado] -----&gt; [Permisos]\n   |\n   v\n[Recurso (Proyecto, Bucket, Modelo...)]\n</code></pre>"},{"location":"unit1/roles/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Tipos de roles en IAM</li> <li>Crear roles personalizados</li> <li>IAM Policy JSON</li> </ul>"},{"location":"unit1/roles/#conclusion","title":"Conclusi\u00f3n","text":"<p>Conocer los distintos tipos de roles te permitir\u00e1 aplicar pol\u00edticas de acceso m\u00e1s efectivas. Usar roles predefinidos y personalizar solo cuando sea necesario ayuda a mantener tu infraestructura segura y manejable.</p> <p>En el siguiente cap\u00edtulo aprender\u00e1s a trabajar con cuentas de servicio, esenciales para tareas automatizadas y pipelines de ML.</p>"},{"location":"unit1/service-accounts/","title":"Cuentas de Servicio","text":""},{"location":"unit1/service-accounts/#introduccion","title":"Introducci\u00f3n","text":"<p>Las cuentas de servicio son un tipo especial de identidad en Google Cloud usadas por aplicaciones, pipelines y servicios automatizados para interactuar con recursos sin intervenci\u00f3n humana.</p> <p>Son esenciales para proyectos de Machine Learning que requieren procesamiento aut\u00f3nomo, ejecuci\u00f3n de pipelines, entrenamiento o despliegue de modelos.</p>"},{"location":"unit1/service-accounts/#por-que-usar-cuentas-de-servicio","title":"\u00bfPor qu\u00e9 usar cuentas de servicio?","text":"<p>Usar cuentas de servicio permite:</p> <ul> <li>\ud83d\udd12 Asegurar que solo las aplicaciones autorizadas accedan a los recursos.</li> <li>\ud83d\udd01 Automatizar tareas como cargas de datos o entrenamiento de modelos.</li> <li>\ud83e\udde0 Ejecutar pipelines en Vertex AI o Workflows de forma segura.</li> </ul>"},{"location":"unit1/service-accounts/#caracteristicas-clave","title":"Caracter\u00edsticas clave","text":"<ul> <li> <p>Cada cuenta tiene una direcci\u00f3n como:   <pre><code>my-pipeline@mi-proyecto.iam.gserviceaccount.com\n</code></pre></p> </li> <li> <p>Se pueden otorgar roles espec\u00edficos a cada cuenta seg\u00fan su funci\u00f3n.</p> </li> <li> <p>Se pueden utilizar desde:</p> </li> <li>Jobs de Dataflow o Dataproc</li> <li>Vertex AI Pipelines</li> <li>Contenedores en Cloud Run o GKE</li> <li>Scripts con SDK de GCP (Python, Go, etc.)</li> </ul>"},{"location":"unit1/service-accounts/#crear-una-cuenta-de-servicio","title":"Crear una cuenta de servicio","text":"<pre><code>gcloud iam service-accounts create mi-pipeline \\\n  --description=\"Cuenta para pipeline de entrenamiento\" \\\n  --display-name=\"pipeline-ml\"\n</code></pre>"},{"location":"unit1/service-accounts/#asignar-un-rol","title":"Asignar un rol","text":"<pre><code>gcloud projects add-iam-policy-binding mi-proyecto-id \\\n  --member=\"serviceAccount:mi-pipeline@mi-proyecto.iam.gserviceaccount.com\" \\\n  --role=\"roles/bigquery.dataViewer\"\n</code></pre> <p>Esto otorga permiso de lectura sobre BigQuery para esa cuenta de servicio.</p>"},{"location":"unit1/service-accounts/#usar-una-cuenta-de-servicio-desde-python","title":"Usar una cuenta de servicio desde Python","text":"<pre><code>from google.cloud import storage\n\n# Autenticaci\u00f3n expl\u00edcita con una cuenta de servicio\nclient = storage.Client.from_service_account_json(\"mi-cuenta.json\")\nbucket = client.get_bucket(\"mi-bucket\")\nprint(bucket.list_blobs())\n</code></pre> <p>\ud83d\udca1 Aseg\u00farate de que el archivo <code>.json</code> de credenciales nunca se suba a GitHub.</p>"},{"location":"unit1/service-accounts/#cuentas-por-entorno","title":"Cuentas por entorno","text":"Entorno / Uso Cuenta sugerida Rol t\u00edpico Vertex AI Pipelines <code>vertex-pipeline@proyecto.iam.gserviceaccount.com</code> <code>roles/aiplatform.user</code> Cloud Functions <code>cf-model-trigger@proyecto.iam.gserviceaccount.com</code> <code>roles/storage.objectViewer</code> Dataflow <code>dataflow-runner@proyecto.iam.gserviceaccount.com</code> <code>roles/dataflow.worker</code> GKE <code>gke-ingestor@proyecto.iam.gserviceaccount.com</code> <code>roles/container.admin</code>"},{"location":"unit1/service-accounts/#buenas-practicas","title":"Buenas pr\u00e1cticas","text":"<ul> <li>\ud83d\udd10 Usa una cuenta por proceso o servicio.</li> <li>\ud83d\uddc2\ufe0f Organiza cuentas con nombres claros y prop\u00f3sito espec\u00edfico.</li> <li>\ud83d\udcc4 Registra qu\u00e9 roles tiene cada cuenta y por qu\u00e9.</li> <li>\ud83d\udeab Nunca uses cuentas de servicio con roles excesivos como <code>Owner</code>.</li> </ul>"},{"location":"unit1/service-accounts/#diagrama-uso-tipico","title":"Diagrama: uso t\u00edpico","text":"<pre><code>Vertex AI Pipeline\n      |\n      v\nService Account (vertex-pipeline@...)\n      |\n      v\nBigQuery  --- Cloud Storage --- Vertex Model Registry\n</code></pre>"},{"location":"unit1/service-accounts/#referencias-utiles","title":"Referencias \u00fatiles","text":"<ul> <li>Documentaci\u00f3n oficial de cuentas de servicio</li> <li>Autenticaci\u00f3n con Python</li> <li>Recomendaciones de seguridad IAM</li> </ul>"},{"location":"unit1/service-accounts/#conclusion","title":"Conclusi\u00f3n","text":"<p>Las cuentas de servicio son la base de una arquitectura automatizada y segura en GCP. Usarlas correctamente te permite orquestar flujos de ML sin comprometer la seguridad ni depender de accesos manuales.</p> <p>En el siguiente cap\u00edtulo revisaremos buenas pr\u00e1cticas para dise\u00f1ar pol\u00edticas de acceso efectivas y seguras.</p>"},{"location":"unit2/","title":"\ud83d\udd25 Fundamentos de PySpark","text":"<p>La Unidad 1 te introduce al mundo del procesamiento distribuido con Apache Spark usando Python. Aqu\u00ed aprender\u00e1s a manejar grandes vol\u00famenes de datos de manera eficiente, utilizando transformaciones, acciones y consultas SQL sobre estructuras distribuidas. Esta unidad es clave para trabajar con Big Data de forma escalable y profesional.</p>"},{"location":"unit2/#objetivos-de-la-unidad","title":"\ud83c\udfaf Objetivos de la unidad","text":"<ul> <li>Comprender el modelo de ejecuci\u00f3n distribuido de Spark.</li> <li>Crear y transformar estructuras de datos distribuidas (RDD y DataFrame).</li> <li>Ejecutar operaciones con <code>transformations</code> y <code>actions</code>.</li> <li>Consultar datos usando Spark SQL.</li> <li>Comprender el flujo de trabajo de un job de Spark.</li> <li>Integrar Spark en entornos locales o en la nube.</li> </ul>"},{"location":"unit2/#contenidos","title":"\ud83d\udcda Contenidos","text":"Tema Descripci\u00f3n breve \u2699\ufe0f Introducci\u00f3n a Spark y PySpark Qu\u00e9 es Spark, c\u00f3mo funciona y c\u00f3mo lo usamos con Python. \ud83d\udce6 RDD y DataFrames Tipos de estructuras distribuidas, diferencias y casos de uso. \ud83d\udd01 Transformaciones y Acciones C\u00f3mo manipular y procesar los datos de forma eficiente. \ud83e\udde0 Lazy Evaluation Ejecuci\u00f3n diferida y optimizaci\u00f3n autom\u00e1tica del plan de ejecuci\u00f3n. \ud83d\udd0d Spark SQL Consultas tipo SQL sobre grandes conjuntos de datos. \ud83e\uddea Spark Streaming (b\u00e1sico) Introducci\u00f3n al procesamiento de datos en tiempo real."},{"location":"unit2/#recomendacion","title":"\ud83e\udde0 Recomendaci\u00f3n","text":"<p>Te sugerimos comenzar por la introducci\u00f3n conceptual y luego avanzar paso a paso por las transformaciones, SQL y finalmente streaming. Aseg\u00farate de ejecutar los ejemplos t\u00fa mismo para entender c\u00f3mo Spark distribuye las operaciones.</p>"},{"location":"unit2/#prerrequisitos","title":"\ud83d\udcda Prerrequisitos","text":"<ul> <li>Conocimientos intermedios de Python</li> <li>Familiaridad con estructuras tipo <code>DataFrame</code></li> <li>Entorno configurado para ejecutar PySpark</li> </ul>"},{"location":"unit2/#que-lograras-al-finalizar-esta-unidad","title":"\u2705 \u00bfQu\u00e9 lograr\u00e1s al finalizar esta unidad?","text":"<ul> <li>Procesar grandes vol\u00famenes de datos con PySpark.</li> <li>Utilizar transformaciones y acciones de forma eficiente.</li> <li>Consultar datos con Spark SQL.</li> <li>Comprender c\u00f3mo funciona un motor de ejecuci\u00f3n distribuido.</li> </ul>"},{"location":"unit3/","title":"\ud83d\udd25 Fundamentos de PySpark","text":"<p>La Unidad 1 te introduce al mundo del procesamiento distribuido con Apache Spark usando Python. Aqu\u00ed aprender\u00e1s a manejar grandes vol\u00famenes de datos de manera eficiente, utilizando transformaciones, acciones y consultas SQL sobre estructuras distribuidas. Esta unidad es clave para trabajar con Big Data de forma escalable y profesional.</p>"},{"location":"unit3/#objetivos-de-la-unidad","title":"\ud83c\udfaf Objetivos de la unidad","text":"<ul> <li>Comprender el modelo de ejecuci\u00f3n distribuido de Spark.</li> <li>Crear y transformar estructuras de datos distribuidas (RDD y DataFrame).</li> <li>Ejecutar operaciones con <code>transformations</code> y <code>actions</code>.</li> <li>Consultar datos usando Spark SQL.</li> <li>Comprender el flujo de trabajo de un job de Spark.</li> <li>Integrar Spark en entornos locales o en la nube.</li> </ul>"},{"location":"unit3/#contenidos","title":"\ud83d\udcda Contenidos","text":"Tema Descripci\u00f3n breve \u2699\ufe0f Introducci\u00f3n a Spark y PySpark Qu\u00e9 es Spark, c\u00f3mo funciona y c\u00f3mo lo usamos con Python. \ud83d\udce6 RDD y DataFrames Tipos de estructuras distribuidas, diferencias y casos de uso. \ud83d\udd01 Transformaciones y Acciones C\u00f3mo manipular y procesar los datos de forma eficiente. \ud83e\udde0 Lazy Evaluation Ejecuci\u00f3n diferida y optimizaci\u00f3n autom\u00e1tica del plan de ejecuci\u00f3n. \ud83d\udd0d Spark SQL Consultas tipo SQL sobre grandes conjuntos de datos. \ud83e\uddea Spark Streaming (b\u00e1sico) Introducci\u00f3n al procesamiento de datos en tiempo real."},{"location":"unit3/#recomendacion","title":"\ud83e\udde0 Recomendaci\u00f3n","text":"<p>Te sugerimos comenzar por la introducci\u00f3n conceptual y luego avanzar paso a paso por las transformaciones, SQL y finalmente streaming. Aseg\u00farate de ejecutar los ejemplos t\u00fa mismo para entender c\u00f3mo Spark distribuye las operaciones.</p>"},{"location":"unit3/#prerrequisitos","title":"\ud83d\udcda Prerrequisitos","text":"<ul> <li>Conocimientos intermedios de Python</li> <li>Familiaridad con estructuras tipo <code>DataFrame</code></li> <li>Entorno configurado para ejecutar PySpark</li> </ul>"},{"location":"unit3/#que-lograras-al-finalizar-esta-unidad","title":"\u2705 \u00bfQu\u00e9 lograr\u00e1s al finalizar esta unidad?","text":"<ul> <li>Procesar grandes vol\u00famenes de datos con PySpark.</li> <li>Utilizar transformaciones y acciones de forma eficiente.</li> <li>Consultar datos con Spark SQL.</li> <li>Comprender c\u00f3mo funciona un motor de ejecuci\u00f3n distribuido.</li> </ul>"},{"location":"unit4/","title":"\ud83d\udd25 Fundamentos de PySpark","text":"<p>La Unidad 1 te introduce al mundo del procesamiento distribuido con Apache Spark usando Python. Aqu\u00ed aprender\u00e1s a manejar grandes vol\u00famenes de datos de manera eficiente, utilizando transformaciones, acciones y consultas SQL sobre estructuras distribuidas. Esta unidad es clave para trabajar con Big Data de forma escalable y profesional.</p>"},{"location":"unit4/#objetivos-de-la-unidad","title":"\ud83c\udfaf Objetivos de la unidad","text":"<ul> <li>Comprender el modelo de ejecuci\u00f3n distribuido de Spark.</li> <li>Crear y transformar estructuras de datos distribuidas (RDD y DataFrame).</li> <li>Ejecutar operaciones con <code>transformations</code> y <code>actions</code>.</li> <li>Consultar datos usando Spark SQL.</li> <li>Comprender el flujo de trabajo de un job de Spark.</li> <li>Integrar Spark en entornos locales o en la nube.</li> </ul>"},{"location":"unit4/#contenidos","title":"\ud83d\udcda Contenidos","text":"Tema Descripci\u00f3n breve \u2699\ufe0f Introducci\u00f3n a Spark y PySpark Qu\u00e9 es Spark, c\u00f3mo funciona y c\u00f3mo lo usamos con Python. \ud83d\udce6 RDD y DataFrames Tipos de estructuras distribuidas, diferencias y casos de uso. \ud83d\udd01 Transformaciones y Acciones C\u00f3mo manipular y procesar los datos de forma eficiente. \ud83e\udde0 Lazy Evaluation Ejecuci\u00f3n diferida y optimizaci\u00f3n autom\u00e1tica del plan de ejecuci\u00f3n. \ud83d\udd0d Spark SQL Consultas tipo SQL sobre grandes conjuntos de datos. \ud83e\uddea Spark Streaming (b\u00e1sico) Introducci\u00f3n al procesamiento de datos en tiempo real."},{"location":"unit4/#recomendacion","title":"\ud83e\udde0 Recomendaci\u00f3n","text":"<p>Te sugerimos comenzar por la introducci\u00f3n conceptual y luego avanzar paso a paso por las transformaciones, SQL y finalmente streaming. Aseg\u00farate de ejecutar los ejemplos t\u00fa mismo para entender c\u00f3mo Spark distribuye las operaciones.</p>"},{"location":"unit4/#prerrequisitos","title":"\ud83d\udcda Prerrequisitos","text":"<ul> <li>Conocimientos intermedios de Python</li> <li>Familiaridad con estructuras tipo <code>DataFrame</code></li> <li>Entorno configurado para ejecutar PySpark</li> </ul>"},{"location":"unit4/#que-lograras-al-finalizar-esta-unidad","title":"\u2705 \u00bfQu\u00e9 lograr\u00e1s al finalizar esta unidad?","text":"<ul> <li>Procesar grandes vol\u00famenes de datos con PySpark.</li> <li>Utilizar transformaciones y acciones de forma eficiente.</li> <li>Consultar datos con Spark SQL.</li> <li>Comprender c\u00f3mo funciona un motor de ejecuci\u00f3n distribuido.</li> </ul>"},{"location":"unit5/","title":"\ud83d\udd25 Fundamentos de PySpark","text":"<p>La Unidad 1 te introduce al mundo del procesamiento distribuido con Apache Spark usando Python. Aqu\u00ed aprender\u00e1s a manejar grandes vol\u00famenes de datos de manera eficiente, utilizando transformaciones, acciones y consultas SQL sobre estructuras distribuidas. Esta unidad es clave para trabajar con Big Data de forma escalable y profesional.</p>"},{"location":"unit5/#objetivos-de-la-unidad","title":"\ud83c\udfaf Objetivos de la unidad","text":"<ul> <li>Comprender el modelo de ejecuci\u00f3n distribuido de Spark.</li> <li>Crear y transformar estructuras de datos distribuidas (RDD y DataFrame).</li> <li>Ejecutar operaciones con <code>transformations</code> y <code>actions</code>.</li> <li>Consultar datos usando Spark SQL.</li> <li>Comprender el flujo de trabajo de un job de Spark.</li> <li>Integrar Spark en entornos locales o en la nube.</li> </ul>"},{"location":"unit5/#contenidos","title":"\ud83d\udcda Contenidos","text":"Tema Descripci\u00f3n breve \u2699\ufe0f Introducci\u00f3n a Spark y PySpark Qu\u00e9 es Spark, c\u00f3mo funciona y c\u00f3mo lo usamos con Python. \ud83d\udce6 RDD y DataFrames Tipos de estructuras distribuidas, diferencias y casos de uso. \ud83d\udd01 Transformaciones y Acciones C\u00f3mo manipular y procesar los datos de forma eficiente. \ud83e\udde0 Lazy Evaluation Ejecuci\u00f3n diferida y optimizaci\u00f3n autom\u00e1tica del plan de ejecuci\u00f3n. \ud83d\udd0d Spark SQL Consultas tipo SQL sobre grandes conjuntos de datos. \ud83e\uddea Spark Streaming (b\u00e1sico) Introducci\u00f3n al procesamiento de datos en tiempo real."},{"location":"unit5/#recomendacion","title":"\ud83e\udde0 Recomendaci\u00f3n","text":"<p>Te sugerimos comenzar por la introducci\u00f3n conceptual y luego avanzar paso a paso por las transformaciones, SQL y finalmente streaming. Aseg\u00farate de ejecutar los ejemplos t\u00fa mismo para entender c\u00f3mo Spark distribuye las operaciones.</p>"},{"location":"unit5/#prerrequisitos","title":"\ud83d\udcda Prerrequisitos","text":"<ul> <li>Conocimientos intermedios de Python</li> <li>Familiaridad con estructuras tipo <code>DataFrame</code></li> <li>Entorno configurado para ejecutar PySpark</li> </ul>"},{"location":"unit5/#que-lograras-al-finalizar-esta-unidad","title":"\u2705 \u00bfQu\u00e9 lograr\u00e1s al finalizar esta unidad?","text":"<ul> <li>Procesar grandes vol\u00famenes de datos con PySpark.</li> <li>Utilizar transformaciones y acciones de forma eficiente.</li> <li>Consultar datos con Spark SQL.</li> <li>Comprender c\u00f3mo funciona un motor de ejecuci\u00f3n distribuido.</li> </ul>"},{"location":"unit6/","title":"\ud83d\udd25 Fundamentos de PySpark","text":"<p>La Unidad 1 te introduce al mundo del procesamiento distribuido con Apache Spark usando Python. Aqu\u00ed aprender\u00e1s a manejar grandes vol\u00famenes de datos de manera eficiente, utilizando transformaciones, acciones y consultas SQL sobre estructuras distribuidas. Esta unidad es clave para trabajar con Big Data de forma escalable y profesional.</p>"},{"location":"unit6/#objetivos-de-la-unidad","title":"\ud83c\udfaf Objetivos de la unidad","text":"<ul> <li>Comprender el modelo de ejecuci\u00f3n distribuido de Spark.</li> <li>Crear y transformar estructuras de datos distribuidas (RDD y DataFrame).</li> <li>Ejecutar operaciones con <code>transformations</code> y <code>actions</code>.</li> <li>Consultar datos usando Spark SQL.</li> <li>Comprender el flujo de trabajo de un job de Spark.</li> <li>Integrar Spark en entornos locales o en la nube.</li> </ul>"},{"location":"unit6/#contenidos","title":"\ud83d\udcda Contenidos","text":"Tema Descripci\u00f3n breve \u2699\ufe0f Introducci\u00f3n a Spark y PySpark Qu\u00e9 es Spark, c\u00f3mo funciona y c\u00f3mo lo usamos con Python. \ud83d\udce6 RDD y DataFrames Tipos de estructuras distribuidas, diferencias y casos de uso. \ud83d\udd01 Transformaciones y Acciones C\u00f3mo manipular y procesar los datos de forma eficiente. \ud83e\udde0 Lazy Evaluation Ejecuci\u00f3n diferida y optimizaci\u00f3n autom\u00e1tica del plan de ejecuci\u00f3n. \ud83d\udd0d Spark SQL Consultas tipo SQL sobre grandes conjuntos de datos. \ud83e\uddea Spark Streaming (b\u00e1sico) Introducci\u00f3n al procesamiento de datos en tiempo real."},{"location":"unit6/#recomendacion","title":"\ud83e\udde0 Recomendaci\u00f3n","text":"<p>Te sugerimos comenzar por la introducci\u00f3n conceptual y luego avanzar paso a paso por las transformaciones, SQL y finalmente streaming. Aseg\u00farate de ejecutar los ejemplos t\u00fa mismo para entender c\u00f3mo Spark distribuye las operaciones.</p>"},{"location":"unit6/#prerrequisitos","title":"\ud83d\udcda Prerrequisitos","text":"<ul> <li>Conocimientos intermedios de Python</li> <li>Familiaridad con estructuras tipo <code>DataFrame</code></li> <li>Entorno configurado para ejecutar PySpark</li> </ul>"},{"location":"unit6/#que-lograras-al-finalizar-esta-unidad","title":"\u2705 \u00bfQu\u00e9 lograr\u00e1s al finalizar esta unidad?","text":"<ul> <li>Procesar grandes vol\u00famenes de datos con PySpark.</li> <li>Utilizar transformaciones y acciones de forma eficiente.</li> <li>Consultar datos con Spark SQL.</li> <li>Comprender c\u00f3mo funciona un motor de ejecuci\u00f3n distribuido.</li> </ul>"}]}